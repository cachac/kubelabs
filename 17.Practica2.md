# 1. Práctica #2 Full Demo <!-- omit in toc -->

> Escenario:

Se ha modernizado una aplicación monolítica separando sus componentes en microservicios, esta se compone de 4 aplicaciones que se comunican entre si.

Las imágenes de estos servicios ya fueron construidas y ahora tienen un requerimiento de infraestructura que soporte un sistema orquestado en contenedores.

Las aplicaciones se acceden por medio del dominio, donde USERLAB es el nombre de laboratorio asignado a cada estudiante, ejemplo: carlosxyz.kubelabs.com

A continuación los requerimientos específicos de cada uno de los módulos.

# 1. Configuracion General

## 1.1. Todos los objetos Kubernetes deben ser creados en archivos YAML, con el objetivo de recrear la solución de forma automatizada y con el mínimo esfuerzo.

## 1.2. Eliminar los INGRESS y otros recursos de laboratorios anteriores
```
k get ing -A
```
## 1.3. Opcional: Instalar las extensiones de VSCode: YAML y Kubernetes.
## 1.4. Opcional: Utilizar Kubens para moverse entre Namespaces
## 1.5. Opcional: Crear una carpeta para los archivos yaml con el nombre de "proyecto-final"

# 2. Namespaces
Es requerido implementar dos grupos de objetos separados por los namespaces:
> public

> private

<details>
  <summary>Tips</summary>

  > [namespace](./assets/namespace-definition.yml)
</details>

# 3. Web Server

## 3.1. Debe ser creado en el namespace ***public***

## 3.2. Dirección de la imagen:
> cachac/kubelabs_webapp:1.1.7

## 3.3. Su contenedor escucha por el puerto 8080.

## 3.4. Para efectos de alta disponibilidad se requieren 2 replicas

## 3.5. Debe ser accedida por Ingress en el dominio <USERLAB>.kubelabs.<TLD> (online ó .dev)

## 3.6. Necesita una ruta apuntando a su root (/)

## 3.7. La página requiere montar un archivo de configuración con las siguientes características:
### 3.7.1. Ubique del archivo en la máquina virtual:
> /home/kube/kubelabs/config.js

### 3.7.2. Edite el archivo config.js agregando el subdominio de cada estudiante
> Sustituir las XXXXXXX con el nombre del SUBDOMINIO y el TLD (.online ó .dev) según sea necesario

### 3.7.3. Dentro del POD montar el archivo en la ruta:
> /usr/share/nginx/html/config.js

<details>
  <summary>Tips</summary>


> [Deployment](./assets/deployment-definition.yml)
>
> [ingress](./assets/ingress-definition.yml)
>
> [Montar ConfigMap en Deployment Volumen (ver lineas 30 a 41)](./assets/deploy-configmap-nginx.yml)
>
> [ConfigMap](./assets/x/05.cm-webpage.yml)

</details>

# 4. Web Socket
## 4.1. Namespace ***public***

## 4.2. Este servicio se accede por la ruta websocket.<USERLAB>.kubelabs.<TLD>
## 4.3. La ruta es /graphql

## 4.4. Este es un servicio CRÍTICO que usa una variable de entorno para establecer la conexión con la página web:

> TOKEN_SECRET: PASS

## 4.5. Puerto 3001

## 4.6. Replicas: 1

## 4.7. Imagen:
> cachac/kubelabs_websocket:1.0.6

<details>
  <summary>Tips</summary>

> [secret](./assets/secret-definition.yml)
>
> [Pod secret linea 31](./assets/15/secret-demo-mysql.yml)
</details>

~~~~
								     				CHECK POINT #1
~~~~

# 5. Public API

## 5.1. Namespace ***public***

## 5.2. Este servicio se accede por la ruta api.<USERLAB>.kubelabs.<TLD>
## 5.3. La ruta es /graphql

## 5.4. Puerto 3000

## 5.5. Replicas: 2

## 5.6. Imagen:
> cachac/kubelabs_public_api:1.0.0

# 6. Private API

## 6.1. Namespace ***private***

## 6.2. Este es servicio privado que no se publica por Internet. Se accede a él únicamente a través del public api, esta conexión se debe configurar en el punto 7.

## 6.3. Puerto 3002

## 6.4. Replicas: 1

## 6.5. Imagen:
> cachac/kubelabs_privateapi:1.0.2

# 7. Establecer la comunicación entre API público y privado

## 7.1. El PUBLIC API utiliza una variable de entorno para establecer la conexión con el API privado:
> Crear la variable como configMap en Namespace PUBLIC y con los siguientes datos:

Debe cambiar los valores del SERVICE y NAMESPACE en el link del Config Map

> PRIVATE_API: http://SERVICE_NAME.NAMESPACE.svc.cluster.local:3002/private


> Nota: /private, es la ruta del REST API, no se debe cambiar

## 7.2. Asignar el CM como variable de entorno en el PUBLIC API
<details>
  <summary>Tips</summary>

  > [configMap](./assets/configmap-definition.yml)
  >
	> [Asignar configMap al POD linea 22](./assets/deploy-configmap.yml)
</details>

~~~~
								     				CHECK POINT #2
~~~~
# 8. Recursos
## 8.1. Página web
> limits: CPU: 100m; memoria: 100Mi
>
> requests: cpu: 10m; memoria: 50Mi

## 8.2. Websocket
> limits: CPU: 250m; memoria: 200Mi
>
> requests: cpu: 100m; memoria: 100Mi

## 8.3. Public API
> limits: CPU: 200m; memoria: 200Mi
>
> requests: cpu: 100m; memoria: 100Mi

## 8.4. Private API
> limits: CPU: 200m; memoria: 200Mi
>
> requests: cpu: 100m; memoria: 100Mi

<details>
  <summary>Tips</summary>

  > [recursos](./assets/pod-range-dev.yml)
</details>

# 9. HealthChecks (Readiness/Liveness)

## 9.1. Websocket
> path: /healthcheck
>
>  puerto: 3081
>
> initialDelaySeconds: 10, periodSeconds: 30, timeoutSeconds: 5
## 9.2. Public API
> path: /healthcheck
>
>  puerto: 3080
>
> initialDelaySeconds: 10, periodSeconds: 30, timeoutSeconds: 5
## 9.3. Private API
> path: /healthcheck
>
>  puerto: 3082
>
> initialDelaySeconds: 10, periodSeconds: 30, timeoutSeconds: 5


<details>
  <summary>Tips</summary>

  > [liveness](./assets/pod-liveness.yml)
	> [readiness](./assets/pod-readiness.yml)
</details>


# 10. Estategias de deployment

## 10.1. La página web debe ser desplegada perdiendo hasta el 50% de los pods
## 10.2. Las réplicas del websocket deben ser eliminadas en su totalidad y recreadas con cada despliegue

<details>
  <summary>Tips</summary>

  > [strategy RollingUpdate](./assets/deploy-lifecycle.yml)
</details>

~~~~
								     				CHECKPOINT FINAL
~~~~

# 11. Extras

## 11.1. Logs
Los logs generados por el PUBLIC API deben ser almacenados persistentemente con los siguientes valores:

- Namespace: public
- Storage Class STANDARD
- Escritura y lectura desde un único NODO (RWO)
- Almacenamiento 50Mi
- Ruta del logs en el POD: /app/logs

<details>
  <summary>Tips</summary>

> [storageClass - PVC](./assets/x/16.pvc-public-api.yml)
>
> [volume + volumeMounts lineas 53 a 60](./assets/x/10.public-api.yml)

</details>

## 11.2. HPA
> [info](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)

> [extra info](https://www.kubecost.com/kubernetes-autoscaling/kubernetes-hpa/)

### 11.2.1. Activar Métricas
Validar si las métricas están instaladas
```
kubectl top pods -n public
```
> error: Metrics API not available

### 11.2.2. Si no está instalado el Metrics Server, ejecutar:
> [Instalacion Metrics Server](https://github.com/kubernetes-sigs/metrics-server?tab=readme-ov-file#installation)
```
kubectl apply -f /home/kube/kubelabs/assets/kind-hpa-components/components.yaml

# Comprobar el Pod de Metrics Server Running y Ready 1/1
k get pod -n kube-system -l k8s-app=metrics-server

# Comprobar métricas de pods
kubectl top pods -n public

# Comprobar métricas de nodos
kubectl top nodes
```
### 11.2.3. El API público es un servicio de alta demanda que debe crecer automaticamente cuando:
> el CPU sea superior al 35%

> la memoria sea superior al 50%

> minimo de replicas: 3

> máximo de replicas: 6
<details>
  <summary>Tips</summary>

  > [HPA Definition](./assets/x/17.hpa-public-api.yml)
</details>

### 11.2.4. Validar el HPA
```
k get hpa -n public
```
Resultado:
~~~~
NAME             REFERENCE                      TARGETS           MINPODS   MAXPODS   REPLICAS
hpa-public-api   Deployment/deploy-public-api   0%/35%, 28%/50%   3         6         3
~~~~
> Observar el % de los Targets: 0/35 (CPU) y 28/50 (MEMORIA)

### 11.2.5. Watch Metrics

```
# Watch Metrics Server:
watch kubectl top pod -n public

# Watch HPA
watch kubectl get hpa -n public
```

### 11.2.6. Stress CPU testing:
> Cambiar el -l app=public-api, por la etiqueta asignada a su deployment
```vim
# Exec al pod
kubectl exec -n public -it $(kubectl get pods -l app=public-api -o jsonpath="{.items[0].metadata.name}" -n public) -- sh

# Instalar stress-ng
apk add stress-ng

# ejecutar el stress al CPU por un minuto
stress-ng --matrix 1 -t 1m
```

### 11.2.7. Revisar los "Watch" con las métricas y cantidad de pods
> La cantidad de CPU incrementa

> La cantidad de Pods incrementa a 6

> Después de 1 minuto, el Stress termina y el CPU vuelve a ~1%.

> Despues de unos minutos la cantidad de Pods vuelve a 3


## 11.3. Opcional: Http Request Load Testing
Instalar en su PC (Linux), la herramienta hey para generar el load testing al Public API
```
sudo apt install hey
```

### 11.3.1. Generar el load testing al public api
#### 11.3.1.1. Ejecutar un request de prueba
Desde su PC (Linux)

Este request llega al Public API y este se comunica con el Private API. Debe retornar un objeto con información.

Cambiar los valores de <USER-LAB> y <TLD>
```
  curl --request POST \
  --header 'content-type: application/json' \
  --url 'http://api.<USER-LAB>.kubelabs.<TLD>/graphql' \
  --data '{ "query": "query { User{ checkPrivateApi } }" }'
```
Respuesta:
~~~~
 {"checkPrivateApi":true}
~~~~
#### 11.3.1.2. Ejecutar el load testing inicial
Desde su PC (Linux)

Cambiar los valores de <USER-LAB> y <TLD>
```
hey -m  POST   -T "application/json"  -d '{ "query": "query { User{ checkPrivateApi } }" }' http://api.<USER-LAB>.kubelabs.<TLD>/graphql
```
Respuesta:
~~~~
Status code distribution:
  [200] 200 responses
~~~~
> Se enviaron 200 request y todos retornaron status 200 (ok!!)

### 11.3.2. Ejecutar el load testing por 60 segundos

Cambiar los valores de <USER-LAB> y <TLD>
```
hey -z 60s -q 1000 -c 100 -t 30 -m  POST  -T "application/json"  -d '{ "query": "query { User{ checkPrivateApi } }" }' http://api.<USER-LAB>.kubelabs.<TLD>/graphql
```
> -z 60s  = 60 segundos de ejecucion

> -q 1000 = 1000 rate limit (por segundo por worker)

> -c 100  = 100 usuarios concurrentes

> -t 30    = 30 segundos de timeout por request

### 11.3.3. Validar el consumo de recursos de los Pods
Puede generar un TEST desde el botón de la página web.

Este request puede tardar mas tiempo de lo habitual debido a que el Pod está siendo estresado por HEY y su CPU y Memoria están siendo afectados.

Puede jugar con los varlos -q y -c de HEY para comprobar otros escenarios de Load Testing

Tambien puede alterar los valores de CPU limit del Pod para comprobar su funcionamiento

## 11.4. Opcional: investigar successThreshold & failureThreshold
> [docs](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes)
## 11.5. Opcional: investigar restart policy
> [docs](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)

### 11.5.1. Otros Comandos
```
# Mem Usage:
kubectl exec -n public -it  <POD_NAME> -- cat /sys/fs/cgroup/memory/memory.usage_in_bytes | awk '{ foo = $1 / 1024 / 1024 ; print foo "MB" }'
#CPU Usage:
kubectl exec -n public -it  <POD_NAME> -- cat /sys/fs/cgroup/cpu/cpuacct.usage
```
> https://unix.stackexchange.com/questions/450748/calculating-cpu-usage-of-a-cgroup-over-a-period-of-time

> https://stackoverflow.com/questions/51641310/kubernetes-top-vs-linux-top
